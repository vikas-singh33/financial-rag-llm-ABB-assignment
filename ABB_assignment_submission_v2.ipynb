{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG System — Apple & Tesla 10-K\n",
        "\n",
        "Built a RAG pipeline to answer questions from Apple's 2024\n",
        "and Tesla's 2023 annual SEC filings.\n",
        "\n",
        "PDFs used:\n",
        "- Apple 10-K (fiscal year ended Sep 28, 2024)\n",
        "- Tesla 10-K (fiscal year ended Dec 31, 2023)"
      ],
      "metadata": {
        "id": "h6tvBmLsTA27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing all the libraries\n",
        "!pip install -q pymupdf\n",
        "!pip install -q langchain langchain-community langchain-openai\n",
        "!pip install -q langchain-groq langchain-cohere langchain-chroma\n",
        "!pip install -q rank_bm25 chromadb\n",
        "\n",
        "print(\"All libraries installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqYiMkqYTElL",
        "outputId": "40b94e08-9b08-4b1c-d1f6-bb484191ce8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.4/323.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mAll libraries installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Setup\n",
        "Installing dependencies and setting API keys for OpenAI\n",
        "(embeddings), Groq (LLM) and Cohere (reranker)."
      ],
      "metadata": {
        "id": "jMrl-UwlU7-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System tools + API keys\n",
        "!apt-get install -y poppler-utils tesseract-ocr -q\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"use_your_key\"\n",
        "os.environ[\"GROQ_API_KEY\"]   = \"use_your_key\"\n",
        "os.environ[\"COHERE_API_KEY\"] = \"use_your_key\"\n",
        "\n",
        "keys = [\"OPENAI_API_KEY\", \"GROQ_API_KEY\", \"COHERE_API_KEY\"]\n",
        "for key in keys:\n",
        "    value = os.environ.get(key, \"\")\n",
        "    status = \"Set\" if value and not value.startswith(\"your-\") else \"Missing\"\n",
        "    print(f\"{key}: {status}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNIGUpXmUmF8",
        "outputId": "7fc06367-c1a2-4aae-87ae-60329e9fd99f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
            "OPENAI_API_KEY: Set\n",
            "GROQ_API_KEY: Set\n",
            "COHERE_API_KEY: Set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Imports\n",
        "Loading all required libraries."
      ],
      "metadata": {
        "id": "zBiRoWfhVHG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports\n",
        "import os, json, fitz, time\n",
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_classic.retrievers import EnsembleRetriever\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "import langchain_core, langchain_groq\n",
        "print(\"langchain_core:\", langchain_core.__version__)\n",
        "print(\"langchain_groq:\", langchain_groq.__version__)\n",
        "print(\"All imports successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5zfgL_xUmJR",
        "outputId": "39ff1d96-9c5d-40f7-f59d-31e787720113"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain_core: 1.2.13\n",
            "langchain_groq: 1.1.2\n",
            "All imports successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Document Ingestion\n",
        "Downloading both PDFs directly from SEC URLs and parsing\n",
        "them page by page using PyMuPDF. Page numbers are captured\n",
        "here for citations later."
      ],
      "metadata": {
        "id": "vgGytZO_VYbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download both 10-K PDFs directly from SEC\n",
        "print(\"Downloading Apple 10-K...\")\n",
        "!wget -q \"https://s2.q4cdn.com/470004039/files/doc_earnings/2024/q4/filing/10-Q4-2024-As-Filed.pdf\" -O \"apple_10k.pdf\"\n",
        "print(\"Apple 10-K downloaded!\")\n",
        "\n",
        "print(\"Downloading Tesla 10-K...\")\n",
        "!wget -q \"https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\" -O \"tesla_10k.pdf\"\n",
        "print(\"Tesla 10-K downloaded!\")\n",
        "\n",
        "def parse_pdf(file_path, doc_name):\n",
        "    \"\"\"Parse PDF page by page using PyMuPDF\"\"\"\n",
        "    print(f\"\\nParsing {doc_name}...\")\n",
        "    doc   = fitz.open(file_path)\n",
        "    pages = []\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        text = page.get_text()\n",
        "        if text.strip():\n",
        "            pages.append({\n",
        "                \"text\"  : text,\n",
        "                \"page\"  : page_num + 1,\n",
        "                \"source\": doc_name\n",
        "            })\n",
        "\n",
        "    doc.close()\n",
        "    print(f\"Done! {len(pages)} pages extracted from {doc_name}\")\n",
        "    return pages\n",
        "\n",
        "# Parse both documents\n",
        "apple_pages = parse_pdf(\"apple_10k.pdf\", \"Apple 10-K\")\n",
        "tesla_pages = parse_pdf(\"tesla_10k.pdf\", \"Tesla 10-K\")\n",
        "all_pages   = apple_pages + tesla_pages\n",
        "\n",
        "print(f\"\\nTotal pages extracted: {len(all_pages)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwRXCAQoUmQD",
        "outputId": "236505c8-5730-4350-d64c-8efb859775dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Apple 10-K...\n",
            "Apple 10-K downloaded!\n",
            "Downloading Tesla 10-K...\n",
            "Tesla 10-K downloaded!\n",
            "\n",
            "Parsing Apple 10-K...\n",
            "Done! 121 pages extracted from Apple 10-K\n",
            "\n",
            "Parsing Tesla 10-K...\n",
            "Done! 130 pages extracted from Tesla 10-K\n",
            "\n",
            "Total pages extracted: 251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Chunking\n",
        "Splitting pages into 1000 character chunks with 200 character\n",
        "overlap so context isn't lost at boundaries."
      ],
      "metadata": {
        "id": "-I3bKsEHVuR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split pages into overlapping chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "documents = []\n",
        "for page in all_pages:\n",
        "    chunks = splitter.split_text(page[\"text\"])\n",
        "    for chunk in chunks:\n",
        "        documents.append(Document(\n",
        "            page_content=chunk,\n",
        "            metadata={\n",
        "                \"source\": page[\"source\"],\n",
        "                \"page\"  : page[\"page\"]\n",
        "            }\n",
        "        ))\n",
        "\n",
        "print(f\"Total chunks created: {len(documents)}\")\n",
        "print(f\"\\nSample chunk:\")\n",
        "print(f\"  Source : {documents[10].metadata['source']}\")\n",
        "print(f\"  Page   : {documents[10].metadata['page']}\")\n",
        "print(f\"  Content: {documents[10].page_content[:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edw_lj4SUmSp",
        "outputId": "dbe71ef9-5a27-48da-8a08-d749f74beba1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 1134\n",
            "\n",
            "Sample chunk:\n",
            "  Source : Apple 10-K\n",
            "  Page   : 4\n",
            "  Content: particular years, quarters, months or periods refer to the Company’s fiscal years ended in September and the associated \n",
            "quarters, months and periods of those fiscal years. Each of the terms the “Comp...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Vector Store\n",
        "Embedding all chunks using OpenAI text-embedding-3-small\n",
        "and storing in ChromaDB. Loads from disk if already built."
      ],
      "metadata": {
        "id": "aXJoTdJ3WJ7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or load vector store\n",
        "if os.path.exists(\"db/chroma_db\"):\n",
        "    print(\"Loading existing vector store from disk...\")\n",
        "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vectorstore = Chroma(\n",
        "        persist_directory=\"db/chroma_db\",\n",
        "        embedding_function=embedding_model,\n",
        "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"Loaded! {vectorstore._collection.count()} documents\")\n",
        "else:\n",
        "    print(\"Creating new vector store...\")\n",
        "    print(\"This will take 3-5 minutes...\")\n",
        "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embedding_model,\n",
        "        persist_directory=\"db/chroma_db\",\n",
        "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "    print(f\"Done! {vectorstore._collection.count()} documents stored\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LNXqAwLUmVg",
        "outputId": "fe9976c2-4f56-45a8-b0a4-1e801ee1fe24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new vector store...\n",
            "This will take 3-5 minutes...\n",
            "Done! 1134 documents stored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Retrieval Pipeline\n",
        "Using hybrid search BM25 for exact keyword matches and\n",
        "vector search for semantic similarity. Cohere reranker\n",
        "then picks the top 5 most relevant chunks."
      ],
      "metadata": {
        "id": "xOdcV-ptWVoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up retrieval pipeline and LLM\n",
        "\n",
        "# 1. Vector retriever — semantic search\n",
        "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
        "\n",
        "# 2. BM25 retriever — keyword search\n",
        "bm25_retriever   = BM25Retriever.from_documents(documents)\n",
        "bm25_retriever.k = 15\n",
        "\n",
        "# 3. Hybrid retriever — combines both\n",
        "hybrid_retriever = EnsembleRetriever(\n",
        "    retrievers=[vector_retriever, bm25_retriever],\n",
        "    weights=[0.7, 0.3]\n",
        ")\n",
        "\n",
        "# 4. Cohere reranker — re-scores top results\n",
        "reranker = CohereRerank(model=\"rerank-english-v3.0\", top_n=5)\n",
        "\n",
        "# 5. Groq LLM — Llama 3.3 70B\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "print(\"Vector retriever  : Ready\")\n",
        "print(\"BM25 retriever    : Ready\")\n",
        "print(\"Hybrid retriever  : Ready\")\n",
        "print(\"Cohere reranker   : Ready\")\n",
        "print(\"Groq Llama 3      : Ready\")\n",
        "print(\"\\nAll components ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3sFcfUuUmX2",
        "outputId": "6aa68f87-fd15-4cd5-d5bd-526c0c267a3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector retriever  : Ready\n",
            "BM25 retriever    : Ready\n",
            "Hybrid retriever  : Ready\n",
            "Cohere reranker   : Ready\n",
            "Groq Llama 3      : Ready\n",
            "\n",
            "All components ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: RAG Pipeline\n",
        "The answer_question() function ties everything together.\n",
        "Takes a question, retrieves context, and generates a cited\n",
        "answer using Groq Llama."
      ],
      "metadata": {
        "id": "T0JciYQXcs6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The core RAG function\n",
        "\n",
        "def build_prompt(query, chunks):\n",
        "    \"\"\"Build citation-enforcing prompt\"\"\"\n",
        "    context = \"\"\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        source = chunk.metadata.get(\"source\", \"Unknown\")\n",
        "        page   = chunk.metadata.get(\"page\", \"?\")\n",
        "        context += f\"\\n--- Chunk {i+1} | Source: {source} | Page: {page} ---\\n\"\n",
        "        context += chunk.page_content + \"\\n\"\n",
        "\n",
        "    prompt = f\"\"\"You are a financial document analyst. Answer ONLY using the context below.\n",
        "\n",
        "STRICT RULES:\n",
        "1. Only use information from the provided context chunks.\n",
        "2. Always cite your source as a list like: [\"Apple 10-K\", \"Item 8\", \"p. 28\"]\n",
        "3. If the answer is not in the context, respond with exactly this JSON:\n",
        "   {{\"answer\": \"Not specified in the document.\", \"sources\": []}}\n",
        "4. If the question is about anything outside Apple or Tesla 10-K filings\n",
        "   (e.g. stock price forecasts, current roles as of 2025, building colors),\n",
        "   respond with exactly this JSON:\n",
        "   {{\"answer\": \"This question cannot be answered based on the provided documents.\", \"sources\": []}}\n",
        "5. Never use outside knowledge or make up numbers.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "Respond ONLY with this JSON format, nothing else:\n",
        "{{\n",
        "  \"answer\": \"your answer here\",\n",
        "  \"sources\": [\"document name\", \"section if known\", \"p. PAGE\"]\n",
        "}}\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def answer_question(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Answers a question using the RAG pipeline.\n",
        "    Args:\n",
        "        query (str): The user question about Apple or Tesla 10-K filings.\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"answer\": \"Answer text or refusal message\",\n",
        "            \"sources\": [\"Apple 10-K\", \"Item 8\", \"p. 28\"]\n",
        "        }\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Hybrid retrieval\n",
        "        raw_chunks      = hybrid_retriever.invoke(query)\n",
        "\n",
        "        # Step 2: Rerank\n",
        "        reranked_chunks = reranker.compress_documents(raw_chunks, query)\n",
        "\n",
        "        # Step 3: Build prompt and call LLM\n",
        "        prompt   = build_prompt(query, reranked_chunks)\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        raw_text = response.content.strip()\n",
        "\n",
        "        # Step 4: Clean markdown code blocks if present\n",
        "        if \"```\" in raw_text:\n",
        "            raw_text = raw_text.split(\"```\")[1]\n",
        "            if raw_text.startswith(\"json\"):\n",
        "                raw_text = raw_text[4:]\n",
        "\n",
        "        # Step 5: Fix smart quotes that break JSON parsing\n",
        "        raw_text = raw_text.replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
        "        raw_text = raw_text.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
        "\n",
        "        return json.loads(raw_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"answer\": f\"Error: {str(e)}\", \"sources\": []}\n",
        "\n",
        "\n",
        "# Quick test\n",
        "print(\"Testing answer_question()...\\n\")\n",
        "result = answer_question(\"What was Apple's total revenue for the fiscal year ended September 28, 2024?\")\n",
        "print(json.dumps(result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5889yJnUmae",
        "outputId": "e1b7723f-ca5b-4f4e-af32-fd406efd09dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing answer_question()...\n",
            "\n",
            "{\n",
            "  \"answer\": \"$391,035\",\n",
            "  \"sources\": [\n",
            "    \"Apple 10-K\",\n",
            "    \"Item 8\",\n",
            "    \"p. 32\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Results\n",
        "Running all 13 assignment questions. 7 second delay between\n",
        "calls to stay within Cohere free tier limits.\n",
        "Questions 11, 12, 13 are intentionally unanswerable from\n",
        "the documents, testing out-of-scope refusal."
      ],
      "metadata": {
        "id": "yrAuVMIPc5DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run all 13 assignment questions\n",
        "questions = [\n",
        "    {\"question_id\": 1,  \"question\": \"What was Apple's total revenue for the fiscal year ended September 28, 2024?\"},\n",
        "    {\"question_id\": 2,  \"question\": \"How many shares of common stock were issued and outstanding as of October 18, 2024?\"},\n",
        "    {\"question_id\": 3,  \"question\": \"What is the total amount of term debt (current + non-current) reported by Apple as of September 28, 2024?\"},\n",
        "    {\"question_id\": 4,  \"question\": \"On what date was Apple's 10-K report for 2024 signed and filed with the SEC?\"},\n",
        "    {\"question_id\": 5,  \"question\": \"Does Apple have any unresolved staff comments from the SEC as of this filing? How do you know?\"},\n",
        "    {\"question_id\": 6,  \"question\": \"What was Tesla's total revenue for the year ended December 31, 2023?\"},\n",
        "    {\"question_id\": 7,  \"question\": \"What percentage of Tesla's total revenue in 2023 came from Automotive Sales (excluding Leasing)?\"},\n",
        "    {\"question_id\": 8,  \"question\": \"What is the primary reason Tesla states for being highly dependent on Elon Musk? Answer in one sentence. Do not use apostrophes or special characters.\"},\n",
        "    {\"question_id\": 9,  \"question\": \"What types of vehicles does Tesla currently produce and deliver?\"},\n",
        "    {\"question_id\": 10, \"question\": \"What is the purpose of Tesla's lease pass-through fund arrangements?\"},\n",
        "    {\"question_id\": 11, \"question\": \"What is Tesla's stock price forecast for 2025?\"},\n",
        "    {\"question_id\": 12, \"question\": \"Who is the CFO of Apple as of 2025?\"},\n",
        "    {\"question_id\": 13, \"question\": \"What color is Tesla's headquarters painted?\"}\n",
        "]\n",
        "\n",
        "final_answers = []\n",
        "\n",
        "for item in questions:\n",
        "    print(f\"Answering Q{item['question_id']}...\")\n",
        "    result = answer_question(item[\"question\"])\n",
        "    final_answers.append({\n",
        "        \"question_id\": item[\"question_id\"],\n",
        "        \"answer\"     : result.get(\"answer\", \"Error\"),\n",
        "        \"sources\"    : result.get(\"sources\", [])\n",
        "    })\n",
        "    time.sleep(7)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL ANSWERS:\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(final_answers, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk4UR53ZUmc0",
        "outputId": "ea796d55-22f0-471c-9233-e9f1681dbaad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answering Q1...\n",
            "Answering Q2...\n",
            "Answering Q3...\n",
            "Answering Q4...\n",
            "Answering Q5...\n",
            "Answering Q6...\n",
            "Answering Q7...\n",
            "Answering Q8...\n",
            "Answering Q9...\n",
            "Answering Q10...\n",
            "Answering Q11...\n",
            "Answering Q12...\n",
            "Answering Q13...\n",
            "\n",
            "============================================================\n",
            "FINAL ANSWERS:\n",
            "============================================================\n",
            "[\n",
            "  {\n",
            "    \"question_id\": 1,\n",
            "    \"answer\": \"$391,035\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Item 8\",\n",
            "      \"p. 32\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 2,\n",
            "    \"answer\": \"15,115,823,000\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Page 2\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 3,\n",
            "    \"answer\": \"$96,662\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Item 8\",\n",
            "      \"p. 34\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 4,\n",
            "    \"answer\": \"November 1, 2024\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Page 60\",\n",
            "      \"p. 60\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 5,\n",
            "    \"answer\": \"No, as of this filing, there is an Item 1B. Unresolved Staff Comments, but the page for this item is 17 and there is no information in the provided context about any comments.\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Item 1B\",\n",
            "      \"p. 17\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 6,\n",
            "    \"answer\": \"$96,773\",\n",
            "    \"sources\": [\n",
            "      \"Tesla 10-K\",\n",
            "      \"Consolidated Statements of Operations\",\n",
            "      \"p. 51\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 7,\n",
            "    \"answer\": \"81%\",\n",
            "    \"sources\": [\n",
            "      \"Tesla 10-K\",\n",
            "      \"Page 39\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 8,\n",
            "    \"answer\": \"Tesla is highly dependent on the services of Elon Musk because he is their Chief Executive Officer and Technoking\",\n",
            "    \"sources\": [\n",
            "      \"Tesla 10-K\",\n",
            "      \"Page 22\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 9,\n",
            "    \"answer\": \"Tesla currently produces and delivers the Model 3, Model Y, Model S, Model X, and Cybertruck, as well as the Tesla Semi in pilot production\",\n",
            "    \"sources\": [\n",
            "      \"Tesla 10-K\",\n",
            "      \"Page 5\",\n",
            "      \"p. 5\",\n",
            "      \"Tesla 10-K\",\n",
            "      \"Page 35\",\n",
            "      \"p. 35\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 10,\n",
            "    \"answer\": \"To finance the cost of solar energy systems with investors through arrangements contractually structured as master leases for an initial term ranging between 10 and 25 years.\",\n",
            "    \"sources\": [\n",
            "      \"Tesla 10-K\",\n",
            "      \"Page 82\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 11,\n",
            "    \"answer\": \"This question cannot be answered based on the provided documents.\",\n",
            "    \"sources\": []\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 12,\n",
            "    \"answer\": \"Not specified in the document.\",\n",
            "    \"sources\": []\n",
            "  },\n",
            "  {\n",
            "    \"question_id\": 13,\n",
            "    \"answer\": \"This question cannot be answered based on the provided documents.\",\n",
            "    \"sources\": []\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tow28KAEUmiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11HhRIQpUmkn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}